---
title: "Applying a Multiverse to Habitat Analyses"
author1: "Benjamin Michael Marshall*"
author2: "Alexander Bradley Duthie**"
affiliation1: "Biological and Environmental Sciences, Faculty of Natural Sciences, University of Stirling, Stirling, FK9 4LA, Scotland, UK"
affiliation2: "-"
corresponding1: "benjaminmichaelmarshall@gmail.com"
corresponding2: "alexander.duthie@stir.ac.uk"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    template: main.tex
    keep_tex: true
    highlight: monochrome
    fig_caption: true
    dev: pdf
  bookdown::html_document2:
    theme: yeti
    highlight: monochrome
    fig_caption: true
link-citations: yes
linkcolor: gray
bibliography: [multiverse_refs.bib, packages_refs.bib]
csl: peerj.csl
editor_options: 
  markdown: 
    wrap: sentence
abstract: ---
keywords: Movement ecology, simulation
---

# Multiverse Introduction

<!-- Baldwin et al 2022 has some solutions for secondary analysis -->

## Research flexibilty and the justification for a multiverse approach

Researchers are intrinsically part of the research process [@levins_dialectical_1985; @tang-martinez_history_2020], and our expectations can shape the answers we find [@holman_evidence_2015].
While we can strive to conduct research objectively, there are frequent moments during research that require judgement calls [@steegen_increasing_2016].
Such calls, choices, or decisions occur throughout the research process, encompassing everything from study design (e.g., sample size, sampling intensity, sample stratification) to analysis (e.g., Bayesian or frequentist, ex/inclusion of outliers).
We draw on our own experience and the input of peers to try and ensure the best choices are made to produce robust and reliable results, but we also contend with data, expertise, and interpretability constraints [@liu_paths_2020]. 

Researchers are also influenced by the incentive system around them [@anderson_perverse_2007].
We cannot undertake research in a vacuum; we often require institutions, funders, and scientific journals to produce and share research.
These bodies can influence what research is conducted; they are in a position to incentivise or disincentivise research of certain topics or methodologies [@fanelli_pressures_2010; @smaldino_natural_2016; @ware_significance_2015].
The use of impact factor (and other similar metrics) is an example of citations being used as a measure of quality or research worth.
But when examined closer, the impact factor appears detached from the robustness or reliability of the research [@Brembs2018].
This decrease in robustness can be seen in the increases in effect sizes inflation, p-value misreporting, among a number of other measures of quality [@Brembs2018].
Similarly, novelty has been fetishised by journals to the detriment of replications studies [@forstmeier_detecting_2017; @vinkers_use_2015; @brembs_reliable_2019], despite widespread agreement on the importance of replications [@fraser_role_2020].
There is a bias towards positive, statistically significant results [@cassey_survey_2004; @jennions_publication_2002].
Due to the nature of frequentist statistical significance, a prioritisation of significant results can elevate underpowered studies and boost false positive rates [@albers_problem_2019; @forstmeier_cryptic_2011]. 

Unfortunately, there is evidence that the system of incentives trickle down to impact the judgement calls and decisions of researchers while undertaking research and when publishing those results.
The more detrimental of these decisions have been termed questionable research practices [@fraser_questionable_2018; @bishop_rein_2019].
They chiefly come in three forms: HARKing, cherry-picking, and p-hacking.
Hypothesising After Results Known (HARKing) is where the research can present the results as a confirmatory result, despite originally there being no or contrary hypothesis.
HARKing can sometimes be further enabled and rationalised by hindsight bias, where unexpected results are perceived as more likely once they have been observed [@forstmeier_detecting_2017; @gelman_garden_2013].
Cherry-picking is the removal or non-reporting of data points or (co-)variables, that did not yield significant results.
P-hacking is the repeated use of statistical tests, with different settings, to achieve statistical significance.
Arguably, the existence of p-hacking is enabled by an over-reliance on p-value thresholds, rather than flexible p-value thresholds that are predefined based on effect size of interest, sample size, and desired accuracy of estimation [@lakens_justify_2018].
Questionable research practices can be viewed as methods to achieve a neat, statistically significant, and publishable narrative [@oboyle_chrysalis_2014]; in the worse cases, narratives could be prioritised over transparently reported results.

There is a fear that questionable research practices, and the broader incentives they are connected to, are responsible for the replicability crisis (often referred to as the "reproducibility crisis"). Across many disciplines, there are examples of replication studies being unable to replicate prior research [@open_science_collaboration_estimating_2015; @freedman_economics_2015; @kelly_rate_2019].
Often these replication efforts are conducted with larger sample sizes, or rely on the consolidation of many independent studies (often in the form of meta-analyses).
The implication is not that the original studies were necessarily flawed; but -- in the absence of questionable research practices -- sufficient variation exists in the study subjects to obscure a consistent effect [i.e., variation beyond variation stemming from sampling; @simonsohn_small_2015].

However, variation can also stem from analysis flexibility (i.e., the presence of many ways to analyse the same data to answer the same question).
This flexibility helps enable questionable research practices [@fraser_questionable_2018], and is potentially steered by publication bias [@cassey_survey_2004; @jennions_publication_2002] if results that produce more publishable results are prioritised/rewarded over less exciting but robust results.
Given the prominence of questionable research practices and publication bias, the inconsistencies between initial and replication studies warrant investigation [especially when analysis flexibility is also implicated in potentially flawed replications @bryan_replicator_2019].
It is key to note that analysis flexibility can still lead to variable results in the absence of any undesirable incentives simply as the result of researchers considering different approaches of differing validities for a given dataset [@gelman_garden_2013].

Scientific progress requires building upon past results, and therefore requires confidence in past results. 
Issues arise when subsequent research is based upon weak foundations --i.e., studies with a limited capacity to be replicated because of questionable practices or over-generalisation.
Early significant results can dictate the direction of research and grow resistant to later contradictory results [@barto_dissemination_2012]; therefore, early diagnoses of overly confident results or previously unknown sources of variation becomes a priority.

In medical fields, a lack of replicability comes with direct monetary and well-being costs [@freedman_economics_2015].
Like the medical field, ecological studies can come with well-being costs to the study subjects [e.g., direct surgery/marking of the animal [@Reinert1982; @Winne2006]], as well as impacts on stakeholders stemming from management decisions.
There are fears that the lack of replicability will feed distrust of science more generally [@anvari_replicability_2018].
Therefore, maximising replicability in ecology is key to minimising research waste [@grainger_evidence_2019] and the negative impacts on systems and subjects studied.

The impacts on the study subjects, paired with the often high monetary costs of ecological studies [particularly bio-logging where animals may undergo surgery, @weaver_technology_2021] means that replications can be more difficult to justify.
When paired with fact that ecological systems are complex and in constant flux --often frustrating perfect replications due to changes in space and time [@nakagawa_replicating_2015; @schnitzer_would_2016] --we are left with a distinct lack of direct replications in ecology [@kelly_rate_2019].

The low prevalence of replications in ecology make it difficult to assess the overall irreplicabilty situation in ecology [@kelly_rate_2019]; but there are several examples that suggest irreplicabilty is something ecologists should be wary of  [@clark_ocean_2020; @roche_behavioural_2020; @sanchez-tojar_meta-analysis_2018; @wang_irreproducible_2018].
The potential for irreplicabilty is further supported by evidence of positive publication bias [@fanelli_positive_2010; @fanelli_negative_2012], and links between smaller sample sizes and inflated effect sizes [@lemoine_underappreciated_2016].

In the absence of direct replications, ecology is often left to assess replicability via conceptual replications [@fraser_role_2020] or efforts broadly referred to as quasi-replications [@palmer_quasi-replication_2000].
Replications range in intensity. Direct (or exact) replications are attempts to replicate a tightly defined concept/hypothesis while duplicating of all characteristics of the original study.
Partial replications are a step looser, where the concept/hypothesis tested is less clearly defined (e.g., applicable to a broader scale) but efforts are made to repeat the same methodology.
The most general category are conceptual replications, where the subject and method of study varies from the original study, but the replication targets a the same concept/hypothesis [@kelly_rate_2019; @nakagawa_replicating_2015].
Both partial and conceptual can be classed as quasi-replications if the concept and scale is broadly defined [@nakagawa_replicating_2015].

Conceptual replications are extremely valuable, but rely on our ability to compare replication efforts to previous findings.
An important aspect of those comparisons is accounting for factors differing between the studies that are not salient to the effect of interest [@forstmeier_detecting_2017]; e.g., those linked to sampling differences [@simonsohn_small_2015].
An example of sampling differences leading to differences in final results can be seen in the case of reptile space use.
@silva_reptiles_2020 showed how frequently a reptile was located by a researcher interacted with the space-use estimation method, leading to large differences in area estimates even when using the same estimation method.
What is revealing is not only how the choices during analysis (e.g., choice of area estimation method) impacted results, but how the error introduced by those choices changed depending on the sampling.
It presents a scenario where the _correct_ choice was dependent on preceding decisions when designing the study; therefore, highlighting the need to explore the impacts of multiple decisions simultaneously. 

As seen in the reptile space use example, the choices made by the researcher [researcher degrees of freedom; @simmons_false-positive_2011] is a key source of variation among studies.
It would be advantageous to understand which choices have a significant impact and whether we can account for differences in choice during comparisons.
An understanding of choice could better guide decisions during a study and potentially be used to gauge the robustness of a given dataset in answering a given question.

Research degrees of freedom [or flexibility in analysis; @forstmeier_detecting_2017] have been elegantly demonstrated by a number of "many analysts" studies [e.g., @silberzahn_many_2018; @huntingtonklein_influence_2021].
In these studies, a number of researchers, or research groups, are tasked with answering the same question.
Naturally each participant takes a slightly different approach, both in how the question is interpreted [@auspurg_has_2021], and the analysis approach chosen [@bastiaansen_time_2020; @gelman_garden_2013], resulting is different final results.
The variation in final results can be considered originating from six sources of uncertainty/variation [@hoffmann_multiplicity_2021]: measurement (randomness from the act of measuring), data preprocessing (decisions on data inclusion/exclusion and transforming), parameter (decisions on which parameters used as covariates/predictors), model (decisions on model structure and specification), method (decisions on method choice and parameterisation), and sampling (randomness as a result of sampling a wider population).
Several sources of variation (data preprocessing, parameter, model, and method) are likely to be particularly key to defining researcher degrees of freedom post data collection.
In some cases, the cause behind the variability in results is hard to diagnose [@breznau_observing_2021], or will be less likely to be questioned because of the agreement with existing theory [@gelman_garden_2013].
There are examples where the variation in results is sufficient to change the final conclusions [@salis_how_2021], and others where it alters the strength of an estimated effect [@desbureaux_subjective_2021].
The importance of the effect size variation is context specific, i.e., how variation relates to the overall effect size, and can impact results pertaining to real-world scenarios [@desbureaux_subjective_2021].

## Multiverse analysis

A rising approach to address the unknown impacts of undisclosed researcher degrees of freedom is to fully explore all plausible or reasonable analysis choices open to researchers -- to explore a multiverse of design choices [@steegen_increasing_2016].
This multiverse analysis -- closely linked to vibration of effects [@patel_assessment_2015], multi-model analysis [@young_model_2017], and specification curve analysis [@simonsohn_specification_2020]-- has the potential to demonstrate and quantify the variation stemming from researcher's analyses choices [@rijnhart_assessing_2021].
Choices can include everything from from sample sizes and splits [e.g., @webb_multiverse_2021] to measurement and summary statistics [e.g., @parsons_exploring_2020], but crucially should only include options that are reasonable [@simonsohn_specification_2020; @del_giudice_travelers_2021].
What counts as reasonable is not necessarily simple, and inclusion of irrelevant choices can easily mask important choices because of the multiplicative nature of a branching path network [@del_giudice_travelers_2021] (Fig. \@ref(fig:multiDiagram)).
Construction of a multiverse requires justification of which decisions are treated as variable, and why there is not an _a priori_ and defensible single solution [@del_giudice_travelers_2021].
A multiverse populated with well-justified decisions allows the exploration of which choices inflate variation between analysis universes, while also offering insights into how to deflate variation [e.g., refinement of initial study design, the removal of ambiguities like tightening categories definitions; @steegen_increasing_2016].

```{r multiDiagram, echo=FALSE, out.width='100%', fig.height=5, fig.width=3, fig.align="centre", fig.cap="Diagram showing how multiverse analysis differs from other approaches. Each branch node represents a choice made during aysnalis"}
knitr::include_graphics(here::here("notebook", "ext_images", "Multiverse compared diagram.png"))
```

Ecological systems are complex to study and frustrate replication efforts [@nakagawa_replicating_2015; @schnitzer_would_2016], and in the case of movement ecology, the data analysed (i.e., derived data, such as step length, speed, and turn angle derived from timestamped coordinate data) require multiple stages of preprocessing.
Therefore, multiverse analysis is an avenue to explore causes of variation between studies without the additional costs of practical studies, while also being capable of exploring data processing decisions that may not have immediately apparent impacts on final results.

If the data entropy [the process in which as data ages the chances of irreversible loss increase, @vines_availability_2014] and resistance to data sharing [@tedersoo_data_2021; @miyakawa_no_2020] can be overcome, we will be able to retroactively explore the impact of researcher degrees of freedom on ecological studies [@rijnhart_assessing_2021].
Such retroactive assessment is an attractive option when other methods to explore false positive rates [@hoffmann_multiplicity_2021], such as preregistration and registered reports [@kaplan_likelihood_2015; @scheel_excess_2021], will require more time to yield results.
Ideally we can use multiverse analysis with preregistrations to boost transparency surrounding the inclusion of decisions and the rationale behind others exclusions [@simonsohn_specification_2020; @dragicevic_increasing_2019].
Given the success of meta-analyses to overcome short-comings in the publication record [e.g., p-hacking; @head_extent_2015], multiverse analysis may aid the direction of future research efforts by providing a means of meeting calls to replicate results before collecting more [@nuijten_verify_2018].

However, as not all choices are equally valid, so multiverse analysis cannot simply provide a correct answer [@steegen_increasing_2016] --the "average" result is not necessarily the closest to the truth.
If we were to undertake a multiverse analysis in a scenario with a "known truth", i.e., using a simulated dataset [@bastiaansen_time_2020], we may be able to detect identify the amount of variation from different sources [e.g, biological variation vs study design variation; @breznau_observing_2021], and potentially the systematic biases stemming from specific choices [potentially via Bayesian Casual Forests @bryan_replicator_2019].

Use of simulated data is an established way to explore the robustness of methodologies [@minchin_simulation_1987; @silva_reptiles_2020]; and this project will harness the benefits of simulated data to assess the impacts of researcher degrees of freedom on the results garnered from animal movement datasets via a multiverse approach.


The initial step will be to develop an animal movement simulation where we can input an often of-interest parameter (e.g., preference for a particular habitat type).
The goal of the multiverse will be to run a multitude of sampling/data processing/analysis approaches on that simulated animal movement data, to determine which pathway or series of decisions gets closest to the predefined parameter used to simulate the data initially.
If a sufficiently diverse set of scenarios can be explored in the multiverse, the third step will be applying the now know uncertainty from particular choices to explore whether different choices would have changed conclusions in real-world datasets.

# Methods

## Simulating the Scenarios

Three scenarios - three species - three landscapes
How many individuals each? Populations of 200? Then maybe at the top end all individuals could be sampled, however, unlikely that would be in the real world.
Weighted landscapes translated into three-tier categories
Two years of simulation, one for model, second for validation

## Sampling and Analysis Options

Ultimate number of variations will be dictated by computational costs.

### Sampling

Three species
Number of individuals sampled from pre-simulated population.
```{r samplingNum}
# possible way of generating a range that is skewed to more realistic number while still hitting the extremes
exp(seq(log(1), log(200), length.out = 8))
```
Tracking length - max one year
Tracking frequency - important to consider timing as well
Tracking timing in relation to activity of animal
Tracking consistency - might be very difficult to define systematically

Error is likely not worth the effort right now, too much complexity linked to cleaning

### Analysis

Joo et al - R packages, adeHabaitaths, amt, and possibly ctmcmove
adeHabitaths has the wides designs and compana, possibly another
amt is rsf and ssf - possible extension using Muff et al for pop level ssf
ctmcmove need more investigation

maybe a super naive measure like Simpson's index?

### Validation

comparing estimates to a second year of data

## Hypotheses

sample size helps
frequency helps when closer to decision making time frame
longer tracking is better

# References
