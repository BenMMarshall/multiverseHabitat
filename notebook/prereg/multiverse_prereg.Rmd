---
title: "Applying a Multiverse to Habitat Analyses"
author1: "Benjamin Michael Marshall*"
author2: "Alexander Bradley Duthie**"
affiliation1: "Biological and Environmental Sciences, Faculty of Natural Sciences, University of Stirling, Stirling, FK9 4LA, Scotland, UK"
affiliation2: "-"
corresponding1: "benjaminmichaelmarshall@gmail.com"
corresponding2: "alexander.duthie@stir.ac.uk"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    template: main.tex
    keep_tex: true
    highlight: monochrome
    fig_caption: true
    dev: pdf
  bookdown::html_document2:
    theme: yeti
    highlight: monochrome
    fig_caption: true
link-citations: yes
linkcolor: gray
bibliography: [multiverse_refs.bib, packages_refs.bib]
csl: peerj.csl
editor_options: 
  markdown: 
    wrap: sentence
abstract: ---
keywords: Movement ecology, simulation
---

# Introduction

<!-- Baldwin et al 2022 has some solutions for secondary analysis -->

Researchers are intrinsically part of the research process [@levins_dialectical_1985; @tang-martinez_history_2020], and our expectations can shape the answers we find [@holman_evidence_2015].
While we can strive to conduct research objectively, there are frequent moments during research that require judgement calls [@steegen_increasing_2016].
Such calls, choices, or decisions occur throughout the research process, encompassing everything from study design (e.g., sample size, sampling intensity, sample stratification) to analysis (e.g., Bayesian or frequentist, ex/inclusion of outliers).
We draw on our own experience and the input of peers to try and ensure the best choices are made to produce robust and reliable results, but we also contend with data, expertise, and interpretability constraints [@liu_paths_2020]. 

Researchers are also influenced by the incentive system around them [@anderson_perverse_2007].
We cannot undertake research in a vacuum; we often require institutions, funders, and scientific journals to produce and share research.
These bodies can influence what research is conducted; they are in a position to incentivise or disincentivise research of certain topics or methodologies [@fanelli_pressures_2010; @smaldino_natural_2016; @ware_significance_2015].
The use of impact factor (and other similar metrics) is an example of citations being used as a measure of quality or research worth.
But when examined closer, the impact factor appears detached from the robustness or reliability of the research [@Brembs2018].
This decrease in robustness can be seen in the increases in effect sizes inflation, p-value misreporting, among a number of other measures of quality [@Brembs2018].
Similarly, novelty has been fetishised by journals to the detriment of replications studies [@forstmeier_detecting_2017; @vinkers_use_2015; @brembs_reliable_2019], despite widespread agreement on the importance of replications [@fraser_role_2020].
There is a bias towards positive, statistically significant results [@cassey_survey_2004; @jennions_publication_2002].
Due to the nature of frequentist statistical significance, a prioritisation of significant results can elevate underpowered studies and boost false positive rates [@albers_problem_2019; @forstmeier_cryptic_2011]. 

Unfortunately, there is evidence that the system of incentives trickle down to impact the judgement calls and decisions of researchers while undertaking research and when publishing those results.
The more detrimental of these decisions have been termed questionable research practices [@fraser_questionable_2018; @bishop_rein_2019].
They chiefly come in three forms: HARKing, cherry-picking, and p-hacking.
Hypothesising After Results Known (HARKing) is where the research can present the results as a confirmatory result, despite originally there being no or contrary hypothesis.
HARKing can sometimes be further enabled and rationalised by hindsight bias, where unexpected results are perceived as more likely once they have been observed [@forstmeier_detecting_2017; @gelman_garden_2013].
Cherry-picking is the removal or non-reporting of data points or (co-)variables, that did not yield significant results.
P-hacking is the repeated use of statistical tests, with different settings, to achieve statistical significance.
Arguably, the existence of p-hacking is enabled by an over-reliance on p-value thresholds, rather than flexible p-value thresholds that are predefined based on effect size of interest, sample size, and desired accuracy of estimation [@lakens_justify_2018].
Questionable research practices can be viewed as methods to achieve a neat, statistically significant, and publishable narrative [@oboyle_chrysalis_2014]; in the worse cases, narratives could be prioritised over transparently reported results.

There is a fear that questionable research practices, and the broader incentives they are connected to, are responsible for the replicability crisis (often referred to as the "reproducibility crisis"). Across many disciplines, there are examples of replication studies being unable to replicate prior research [@open_science_collaboration_estimating_2015; @freedman_economics_2015; @kelly_rate_2019].
Often these replication efforts are conducted with larger sample sizes, or rely on the consolidation of many independent studies (often in the form of meta-analyses).
The implication is not that the original studies were necessarily flawed; but -- in the absence of questionable research practices -- sufficient variation exists in the study subjects to obscure a consistent effect [i.e., variation beyond variation stemming from sampling; @simonsohn_small_2015].

However, variation can also stem from analysis flexibility (i.e., the presence of many ways to analyse the same data to answer the same question).
This flexibility helps enable questionable research practices [@fraser_questionable_2018], and is potentially steered by publication bias [@cassey_survey_2004; @jennions_publication_2002] if results that produce more publishable results are prioritised/rewarded over less exciting but robust results.
Given the prominence of questionable research practices and publication bias, the inconsistencies between initial and replication studies warrant investigation [especially when analysis flexibility is also implicated in potentially flawed replications @bryan_replicator_2019].
It is key to note that analysis flexibility can still lead to variable results in the absence of any undesirable incentives simply as the result of researchers considering different approaches of differing validities for a given dataset [@gelman_garden_2013].

Scientific progress requires building upon past results, and therefore requires confidence in past results. 
Issues arise when subsequent research is based upon weak foundations --i.e., studies with a limited capacity to be replicated because of questionable practices or over-generalisation.
Early significant results can dictate the direction of research and grow resistant to later contradictory results [@barto_dissemination_2012]; therefore, early diagnoses of overly confident results or previously unknown sources of variation becomes a priority.

In medical fields, a lack of replicability comes with direct monetary and well-being costs [@freedman_economics_2015].
Like the medical field, ecological studies can come with well-being costs to the study subjects [e.g., direct surgery/marking of the animal [@Reinert1982; @Winne2006]], as well as impacts on stakeholders stemming from management decisions.
There are fears that the lack of replicability will feed distrust of science more generally [@anvari_replicability_2018].
Therefore, maximising replicability in ecology is key to minimising research waste [@grainger_evidence_2019] and the negative impacts on systems and subjects studied.

The impacts on the study subjects, paired with the often high monetary costs of ecological studies [particularly bio-logging where animals may undergo surgery, @weaver_technology_2021] means that replications can be more difficult to justify.
When paired with fact that ecological systems are complex and in constant flux --often frustrating perfect replications due to changes in space and time [@nakagawa_replicating_2015; @schnitzer_would_2016] --we are left with a distinct lack of direct replications in ecology [@kelly_rate_2019].

The low prevalence of replications in ecology make it difficult to assess the overall irreplicabilty situation in ecology [@kelly_rate_2019]; but there are several examples that suggest irreplicabilty is something ecologists should be wary of  [@clark_ocean_2020; @roche_behavioural_2020; @sanchez-tojar_meta-analysis_2018; @wang_irreproducible_2018].
The potential for irreplicabilty is further supported by evidence of positive publication bias [@fanelli_positive_2010; @fanelli_negative_2012], and links between smaller sample sizes and inflated effect sizes [@lemoine_underappreciated_2016].

In the absence of direct replications, ecology is often left to assess replicability via conceptual replications [@fraser_role_2020] or efforts broadly referred to as quasi-replications [@palmer_quasi-replication_2000].
Replications range in intensity. Direct (or exact) replications are attempts to replicate a tightly defined concept/hypothesis while duplicating of all characteristics of the original study.
Partial replications are a step looser, where the concept/hypothesis tested is less clearly defined (e.g., applicable to a broader scale) but efforts are made to repeat the same methodology.
The most general category are conceptual replications, where the subject and method of study varies from the original study, but the replication targets a the same concept/hypothesis [@kelly_rate_2019; @nakagawa_replicating_2015].
Both partial and conceptual can be classed as quasi-replications if the concept and scale is broadly defined [@nakagawa_replicating_2015].

Conceptual replications are extremely valuable, but rely on our ability to compare replication efforts to previous findings.
An important aspect of those comparisons is accounting for factors differing between the studies that are not salient to the effect of interest [@forstmeier_detecting_2017]; e.g., those linked to sampling differences [@simonsohn_small_2015].
An example of sampling differences leading to differences in final results can be seen in the case of reptile space use.
@silva_reptiles_2020 showed how frequently a reptile was located by a researcher interacted with the space-use estimation method, leading to large differences in area estimates even when using the same estimation method.
What is revealing is not only how the choices during analysis (e.g., choice of area estimation method) impacted results, but how the error introduced by those choices changed depending on the sampling.
It presents a scenario where the _correct_ choice was dependent on preceding decisions when designing the study; therefore, highlighting the need to explore the impacts of multiple decisions simultaneously. 

As seen in the reptile space use example, the choices made by the researcher [researcher degrees of freedom; @simmons_false-positive_2011] is a key source of variation among studies.
It would be advantageous to understand which choices have a significant impact and whether we can account for differences in choice during comparisons.
An understanding of choice could better guide decisions during a study and potentially be used to gauge the robustness of a given dataset in answering a given question.

Research degrees of freedom [or flexibility in analysis; @forstmeier_detecting_2017] have been elegantly demonstrated by a number of "many analysts" studies [e.g., @silberzahn_many_2018; @huntingtonklein_influence_2021].
In these studies, a number of researchers, or research groups, are tasked with answering the same question.
Naturally each participant takes a slightly different approach, both in how the question is interpreted [@auspurg_has_2021], and the analysis approach chosen [@bastiaansen_time_2020; @gelman_garden_2013], resulting is different final results.
The variation in final results can be considered originating from six sources of uncertainty/variation [@hoffmann_multiplicity_2021]: measurement (randomness from the act of measuring), data preprocessing (decisions on data inclusion/exclusion and transforming), parameter (decisions on which parameters used as covariates/predictors), model (decisions on model structure and specification), method (decisions on method choice and parameterisation), and sampling (randomness as a result of sampling a wider population).
Several sources of variation (data preprocessing, parameter, model, and method) are likely to be particularly key to defining researcher degrees of freedom post data collection.
In some cases, the cause behind the variability in results is hard to diagnose [@breznau_observing_2021], or will be less likely to be questioned because of the agreement with existing theory [@gelman_garden_2013].
There are examples where the variation in results is sufficient to change the final conclusions [@salis_how_2021], and others where it alters the strength of an estimated effect [@desbureaux_subjective_2021].
The importance of the effect size variation is context specific, i.e., how variation relates to the overall effect size, and can impact results pertaining to real-world scenarios [@desbureaux_subjective_2021].

## Multiverse analysis

A rising approach to address the unknown impacts of undisclosed researcher degrees of freedom is to fully explore all plausible or reasonable analysis choices open to researchers -- to explore a multiverse of design choices [@steegen_increasing_2016].
This multiverse analysis -- closely linked to vibration of effects [@patel_assessment_2015], multi-model analysis [@young_model_2017], and specification curve analysis [@simonsohn_specification_2020]-- has the potential to demonstrate and quantify the variation stemming from researcher's analyses choices [@rijnhart_assessing_2021].
Choices can include everything from from sample sizes and splits [e.g., @webb_multiverse_2021] to measurement and summary statistics [e.g., @parsons_exploring_2020], but crucially should only include options that are reasonable [@simonsohn_specification_2020; @del_giudice_travelers_2021].
What counts as reasonable is not necessarily simple, and inclusion of irrelevant choices can easily mask important choices because of the multiplicative nature of a branching path network [@del_giudice_travelers_2021] (Fig. \@ref(fig:multiDiagram)).
Construction of a multiverse requires justification of which decisions are treated as variable, and why there is not an _a priori_ and defensible single solution [@del_giudice_travelers_2021].
A multiverse populated with well-justified decisions allows the exploration of which choices inflate variation between analysis universes, while also offering insights into how to deflate variation [e.g., refinement of initial study design, the removal of ambiguities like tightening categories definitions; @steegen_increasing_2016].

```{r multiDiagram, echo=FALSE, out.width='100%', fig.height=5, fig.width=3, fig.align="centre", fig.cap="Diagram showing how multiverse analysis differs from other approaches. Each branch node represents a choice made during aysnalis"}
knitr::include_graphics(here::here("notebook", "ext_images", "Multiverse compared diagram.png"))
```

Ecological systems are complex to study and frustrate replication efforts [@nakagawa_replicating_2015; @schnitzer_would_2016], and in the case of movement ecology, the data analysed (i.e., derived data, such as step length, speed, and turn angle derived from timestamped coordinate data) require multiple stages of preprocessing.
Therefore, multiverse analysis is an avenue to explore causes of variation between studies without the additional costs of practical studies, while also being capable of exploring data processing decisions that may not have immediately apparent impacts on final results.

If the data entropy [the process in which as data ages the chances of irreversible loss increase, @vines_availability_2014] and resistance to data sharing [@tedersoo_data_2021; @miyakawa_no_2020] can be overcome, we will be able to retroactively explore the impact of researcher degrees of freedom on ecological studies [@rijnhart_assessing_2021].
Such retroactive assessment is an attractive option when other methods to explore false positive rates [@hoffmann_multiplicity_2021], such as preregistration and registered reports [@kaplan_likelihood_2015; @scheel_excess_2021], will require more time to yield results.
Ideally we can use multiverse analysis with preregistrations to boost transparency surrounding the inclusion of decisions and the rationale behind others exclusions [@simonsohn_specification_2020; @dragicevic_increasing_2019].
Given the success of meta-analyses to overcome short-comings in the publication record [e.g., p-hacking; @head_extent_2015], multiverse analysis may aid the direction of future research efforts by providing a means of meeting calls to replicate results before collecting more [@nuijten_verify_2018].

However, as not all choices are equally valid, so multiverse analysis cannot simply provide a correct answer [@steegen_increasing_2016] --the "average" result is not necessarily the closest to the truth.
If we were to undertake a multiverse analysis in a scenario with a "known truth", i.e., using a simulated dataset [@bastiaansen_time_2020], we may be able to detect identify the amount of variation from different sources [e.g, biological variation vs study design variation; @breznau_observing_2021], and potentially the systematic biases stemming from specific choices [potentially via Bayesian Causal Forests @bryan_replicator_2019].

Use of simulated data is an established way to explore the robustness of methodologies [@minchin_simulation_1987; @silva_reptiles_2020]; and this project will harness the benefits of simulated data to assess the impacts of researcher degrees of freedom on the results garnered from animal movement datasets via a multiverse approach.

Here we describe the initial plan to undertake a multiverse approach to explore how decisions concerning the design and analysis of an animal tracking study can impact the findings on habitat selection.
We describe the simulated scenarios we plan to base the study upon and a preliminary multiverse branching path consisting of three analysis approaches, as well as several broad hypotheses.

<!-- The initial step is run an animal movement simulation where we can input an often of-interest parameter (e.g., preference for a particular habitat type). -->
<!-- The goal of the multiverse will be to run a multitude of sampling/data processing/analysis approaches on that simulated animal movement data, to determine which pathway or series of decisions gets closest to the predefined parameter used to simulate the data initially. -->
<!-- If a sufficiently diverse set of scenarios can be explored in the multiverse, the third step will be applying the now know uncertainty from particular choices to explore whether different choices would have changed conclusions in real-world datasets. -->

# Methods

## Simulating the Scenarios

<!-- Three scenarios - three species - three landscapes -->
We simulated three scenarios/species, comprising of different animals and landscapes.
We simulated the landscapes using the NLMR `r paste0("v.", packageVersion("NLMR"))` package [@NLMR], and the animal movement using abmAnimalMovement `r paste0("v.", packageVersion("abmAnimalMovement"))` [@abmAnimalMovement].
The abmAnimalMovement simulations are a discrete time, agent-based modelling approach for simulating animal movement.
The full details of how the scenarios were parametrised can be found in [@abmAnimalMovement], and also in the abmAnimalMovement github repository ([abmAnimalMovement Github](https://github.com/BenMMarshall/abmAnimalMovement/tree/main/notebook/manuscript)).

In brief:
- Species 1, Badger: site fidelity to two shelter sites, a low movement speed constrained by terrestrial environment and territoriality, an 8-12 hour activity cycle with seasonal shifts
- Species 2, Vulture: medium site fidelity via the use of multiple roosting/resting sites a high and variable movement speed with minimal landscape resistance, an 8-12 hour activity cycle with seasonal shifts
- Species 3, King cobra: lower site fidelity making use of many shelter sites, a medium movement speed through a landscape with high resistance barriers, an 8-12 hour activity cycle combined with a approximately weekly forage-digest cycle and a weak seasonal cycle

<!-- How many individuals each? Populations of 200? Then maybe at the top end all individuals could be sampled, however, unlikely that would be in the real world. -->
Using these setting we simulated a population individuals of each species.
<!-- Two years of simulation, one for model, second for validation -->
Each simulation contained sufficient time steps (i.e., movement choices made by the animal) to provide data for model generation and validation.

<!-- Weighted landscapes translated into three-tier categories -->
Our landscapes comprise of three elements, which are considered by the animal differently depending on the behavioural state the animal is in.
The three elements are matrices where each cell describes either foraging quality, shelter site quality, and movement ease.
The three are non-independent, and based on a single initial random generation using a Gaussian field.

<!-- [MORE SETTINGS HERE?] -->
From the initial Gaussian field we altered the values to exaggerate the difference between high resource areas and low value areas. 
Broadly, we created: core resource areas (e.g., forests) with higher foraging quality but lower movement ease, edge areas that overlap with the forest with better movement ease and also housing higher shelter quality, and more barren areas with high movement ease but with minimal foraging value.

<!-- figure here of landscapes -->
While our landscapes comprise of continuous values that determine the probability of a simulated animal using a cell or not, many habitat selection studies use categoric habitat metrics or land use types.
Therefore, to simply the interpretation of the multiverse we simplify out landscape into three distinct categories for analysis (Fig. \@ref(fig:landscapeExample)): a low quality area where resources are low but movement is easy (class 0 in the figure), an edge habitat with middling resources and high movement ease (class 1), and a high resource habitat with low movement ease (class 2).
<!-- figure here of landscapes turned into categories -->

```{r landscapeExample, echo=FALSE, out.width='100%', fig.height=5, fig.width=3, fig.align="centre", fig.cap="An example of the underlying environmental matricies used during simulation, and the subsequent habitat classes to be used in the habitat selection analysis."}
knitr::include_graphics(here::here("notebook", "prereg", "figures", "landscapeExample.png"))
```

## Sampling and Analysis Options

Ultimately number of variations and choices we are able to explore will be dictated by computational costs and therefore time; however, below we describe the currently planned decision nodes.

### Sampling

As stated we have three species simulations planned using the abmAnimalMovement package [@abmAnimalMovement].
The first decision point of a study would be the number of individuals sampled from that population (i.e., sample size).
However, as many of the habitat selection analyses focus on individual selection we will not explore study sample size initially.
<!-- # ```{r samplingNum} -->
<!-- # # possible way of generating a range that is skewed to more realistic number while still hitting the extremes -->
<!-- # exp(seq(log(1), log(200), length.out = 8)) -->
<!-- # ``` -->

<!-- Tracking length - max one year -->
<!-- Tracking frequency - important to consider timing as well -->
<!-- Tracking timing in relation to activity of animal -->
<!-- Tracking consistency - might be very difficult to define systematically -->
The first decisions concern data quantity.
We aim to vary tracking duration and tracking frequency primarily, while keep consistency fixed.
While tracking consistency, or random/systematic data loss during tracking, is an element affecting data quantity the numerous ways of defining consistency means we will not explore tracking consistency at this time.
Tracking timing is also an important consideration; for example, recording the locations of a diurnal animal only at night is highly unlikely to be a viable way of determining foraging choices.
For this exploration we will assume that the researcher has sufficient knowledge about the animal's ecology to prioritise tracking during active hours.
This only becomes a consideration when tracking frequency lowers to the point where the tracks performed during a day could occur entirely outside the animal's active period.

<!-- Error is likely not worth the effort right now, too much complexity linked to cleaning -->
Data or location quality is another point of sampling variation.
Different bio-logging equipment, terrain, animal behaviour, and weather can all impact the location error when tracking an animal.
As the causes and measurement of error, as well as the solutions to, location error are numerous we will not be exploring the impact of error during this exploration in favour of more variation in other decision nodes.

In summary, the sampling decisions will cover species, tracking frequency, and tracking duration (Fig. \@ref(fig:decisionTrees)A).

```{r decisionTrees, echo=FALSE, out.width='100%', fig.height=5, fig.width=3, fig.align="centre", fig.cap="..."}
knitr::include_graphics(here::here("notebook", "prereg", "figures", "preregDecisionTrees.png"))
```

### Analysis

For the analysis decisions we focus entirely on decisions that can come out of an R analysis workflow.
R presents the number one tool for analysing animal movement data [@joo_recent_2022].
@joo_navigating_2020 provides a review of the R packages available to analysis movement data in R, and we use this review as a resource for determining the options for habitat analysis.
Specifically, we will explore the decisions made during the workflows using adehabitatHS `r paste0("v.", packageVersion("adehabitatHS"))` [@adehabitatHS], amt `r paste0("v.", packageVersion("amt"))` [@amt], and ctmcmove `r paste0("v.", packageVersion("ctmcmove"))` [@ctmcmove] R packages.
Combined the packages offer many options for habitat analysis, we will focus on three.
One from the adehabitatHS package: resource selection ratios (Wides).
<!-- Three from the adehabitatHS package: resource selection ratios (Wides), Compositional Analysis of Habitat Use (Compana), Eigenanalysis of Selection Ratios (Eisera). -->
And two from the amt package: resource selection function (RSF), step selection function (SSF)
<!-- ; and one from the ctmcmove package using a continuous time Markov chain (CTMC) framework. -->
Each of the methods require downstream decisions resulting in a "garden of forking paths" to a final estimate of habitat selection (Fig. \@ref(fig:decisionTrees)B). 

<!-- #### adeHabaitatHS -->
<!-- - define available area -->
<!-- - wides II or III -->
<!-- - compana -->
<!-- - eisera -->
<!-- - engen - no because it doesn't have the II and III choice -->
<!-- - k-select - no because it is better suited to continuous habitat variables using PCA -->

<!-- #### amt -->
<!-- - rsf or ssf -->
<!-- - rsf available area definition -->
<!-- - rsf point weighting -->
<!-- - for higher res data, use of bursts? -->
<!-- - model formula - issf vs ssf, maybe step::interactions -->
<!-- - extract covariates from start end or mid -->
<!-- - number of options -->
<!-- could vary the distributions sl and ta are based on?  -->
<!-- no resampling or burst because we have artificially consistent data collected -->

<!-- #### ctmcmove -->
<!-- - define the sequence of times on which to sample the imputed path -->
<!-- - varying the spacing of the knots -->
<!-- - LinearInterp or ShortestPath -->
<!-- - CAR1 and CAR2 -->

The Wides and RSF methods share many analysis decisions, principally concerned with how available habitat is defined.
<!-- The methods from the adehabitatHS package all require similar choices to be made; -->
The first decisions is whether to approach the habitat selection analysis as a type I, II or III design, as we are concerned about an individuals' habitat selection we will ignore type I.
While type II habitat selection can identify individual preference it requires habitat availability to be defined on a population or landscape scale (i.e., a universal availability).
As movement data presents an opportunity to estimate individual preference, and we are not simulating a true interacting population, we focus on type III.
Type III and the need to individually define available habitat opens up many different analytically decisions concerning how that availability is defined.
<!-- Wides, Compana, and Eisera analyses can be used in either type II or III scenarios. -->

We used a range of methods are build areas from the recorded locations of the animal.
Minimum convex polygons (MCPs) are a simply create a polygon based on the out most locations.
Kernel Density Estimations (KDEs) use kernel smoothing to build a heat map of use or utilisation.
Critical to their operation is a bandwidth or smoothing factor (h) that can alter the resulting area treated as used by the animal [@silva_reptiles_2020].
For this analysis we use the the reference bandwidth (href) as it is commonly used [@crane_lots_2020], while also quickly and consistently calculable on a individual by individual basis.
An alternative method to determine smoothing factor is Least Squares Cross Validation (LSCV).
However, we avoided the LSCV method for two reasons: 1) the LSCV method can fail to converge on a smoothing factor thereby introducing an unpredictable fail state in any potential multiverse, 2) the more limited area estimate it produces make it less suitable for defining habitat availability.

The limitations of MCPs and KDEs have prompted the development of newer methods of area use estimation [@silva_reptiles_2020] that better account for the non-independence and autocorrelative structures within animal movement data [@Noonan2018].
We use two of these methods to define availability.

First is the Autocorrelated Kernel Density Estimations (AKDEs) [@Fleming2015; @Fleming2017] from the ctmm package [@ctmm; @Calabrese2016].
The ctmm package provides a workflow for creating an area estimate based on a number of movement models.
The movement models (Ornstein-Uhlenbeck, Ornsteinâ€“Uhlenbeck Foraging, and Independent Identically Distributed) include different levels of autocorrelative structure.
[BREAKDOWN EACH HERE]
The movement model used would constitute a decision during analysis; however, the ctmm package allows for model comparisons (using AICc) to chose a single best model.
Therefore, we will use the guidance from [@silva_autocorrelationinformed_2022] to generate weighted AKDEs using (PHREML), and select the best performing by AICc for inclusion in further analysis.

The second movement-specific method we will use is dynamic Brownian Bridge Movement Models (dBBMMs) [@Kranstauber2012] from the move package [@move], that estimates movement capacity of the animal to calibrate repeated random walks between known locations.
Unlike the AKDEs, dBBMMs do not produce an utilisation distribution.
As dBBMMs are estimating the uncertainty surrounding potential movement pathways, the resulting distribution is better described as an occurrence distribution.
This occurrence distribution is describing the within-sample uncertainty rather than the potential areas available to the animal beyond the sampling period (i.e., it possess little to no predictive capabilities).
Therefore, using dBBMMs to define habitat availability means rather than comparing the habitat use (derived from the movement data) to availability (broader area predicted by the movement data), we are more closely comparing comparing habitat use to an alternative measure of habitat use.
We included dBBMMs to examine whether this arguably easy-to-make conceptual mistake impacts the final results.
[ALSTON newer paper on unifiying terms needs to be cited here]

DBBMMs require a window and margin size that defines the number of data points over which movement capacity (motion variance) is calculated [@Kranstauber2012]. 
<!-- Fortunately the areas resulting from dBBMMs are largely insensitive to this choice, especially when large windows and margins are used. -->
Window and margin are defined by a number of data points; therefore, to keep the time they represent the same between different tracking frequencies, we changed their values for each tracking frequency. 
As our most infrequent tracking is 168 hours (1 week), we set the window to the number of data points collected over 168 hours, and a margin of 48 hours.
The broader window and margin sizes helped reduce computational costs, a major concern when so many dBBMMs are being calculated.
<!-- In these instances we will report an NA as a final result. -->

<!-- Type II designs require the availability to the same for all animals. -->
<!-- To create population-level availability we will use the above described methods for each individual, then combine the areas into a single polygon. -->
<!-- We will also include a "landscape" description of availability that will include the entire landscape raster. -->

Each of these area estimation methods require a choice to be made regarding the outermost boundary.
MCPs areas are generated based a percentage of the location points; whereas the KDE, AKDE, and dBBMM methods require a contour to be extracted from the utilisation distribution (occurrence distribution for dBBMMs).
We will explore the impact of using a 90, 95, and 99% contour for all the methods.
AKDEs also provide a 95% confidence interval surrounding any chosen contour; for the purposes of simplicity we only use the point estimate for each % contour.

Once we have a defined availability area, we generate points at which the habitat type is recorded thereby estimating the relative availability.
How many and in what alignment these availability points are generated poses two additional points of analytical choice.
For this study we varied the number of availability points as a multiple of the number of animal locations in the dataset: [ADD IN R CODE TO PULL THE CHOICE FROM THE TARGETS SCRIPT].
For the alignment of the points, points where either random or stratified within the available previously defined.

As mentioned resource selection function (RSF) share the above decisions on availability with the Wides methods.
In addition the RSF we also explore the impact of varying the weighting of the available points when the RSF (i.e., generalized linear model / logistic regression) model is run, which impacts the fitting process.
There are suggestions that altering the weighing can improve model performance and decrease the uncertainty surrounding habitat preference estimates [NEED THE FIEBERG CITE HERE]; we include this decision to examine whether the pursuit of a more confident answer is biasing the point estimate.

Other than the area based methods of Wides and RSF, we also include an exploration of Step Selection Functions (SSF).
Instead of estimating habitat availability using area, SSFs use observed step lengths and turn angles to generate available locations at each time step (i.e., for each data point in the dataset; strata).
As a result SSFs have a number of decisions not shared with the area methods.

We explored three decisions associated with the generation of random (available but unused) locations for each step.
The first is similar to the area methods, we varied the number of random locations generated per step from ## to ##.
The second and third are concerning the distributions used to generate the random step and turn angles.
Step lengths were tested with a Gamma and Exponential distributions, while turn angle was tested with Von-Mises and Uniform distributions.

The other decisions we explored in SSFs was whether to run the model as a standard step selection or an integrated step selection function.
The standard step selection model is a conditional logistic regression, where we aim to estimate the used/non-used by the habitat values (case_ ~ values + strata(step_id_)).
The integrated step selection model is very similar, apart from the predictors also included step length and turn angle.
The addition of these two components is meant to reduce bias by better accounting for the selection that may simply be an artifact of the movements of the animal rather than active habitat preference [CITE HERE].

We have neglected to explore one very expansive aspect of SSFs regarding high resolution data and the use of "bursts".
Bursts are used when the researcher wants to look at selection at a given timeframe; therefore. locations are grouped into bursts and the bursts become the strata in the model rather than timestep.
Such grouping of data deserves investigation, but the myriad of ways bursts can be defined and their interaction with tracking duration, frequency, and consistency would warrant a separate multiverse analysis.
As such we have restricted this exploration to step selection functions where the datapoints are equal to the strata in the model.

<!-- The ctmc approach requires a number of decisions regarding the fitting of the quasi-continuous path model, and how that is imputed to a discrete space path. -->
<!-- We will vary the spacing of the knots, the times at which the imputed path is sampled, and the precision matrix used during the path imputation. -->
<!-- We will also vary the method used to convert the imputed path to a continuous-time discrete-space path. -->
<!-- We will explore the impact of Linear Interpretation versus Shortest Path, and Rook's neighbourhood versus King's neighbourhood. -->

<!-- Once all decision pathways have resulted in a value for selection, we will normalise the results so they are directly comparable. -->
<!-- This may require the results to be dichotomised into detection/non-detection of selection for a certain habitat. -->

As the three methods described above produce habitat preference values on different scales and have different decisions associated with them, we elected to analyse the impact of the decisions separately.
For each method we ran two Bayesian regression models to explore: 1) which decisions best explain the absolute deviation from the median estimated preference, 2) which decisions best explain the transformed deviation from the median estimated preference.
The first model provides us with an idea of the decisions that can lead to random, but potentially unbiased noise surrounding the median estimate; whereas the second model highlights decisions that lead to systematic over or under estimation of a habitat preference.

We elected to have the model try and predict the median estimate of habitat preference because the simulation cannot provide a direct analogous value to the outputs of all three methods.
Although the simulation is a discrete time model, where the animal makes decisions based upon a number of options, is similar to a step selection model, the decision making of the animal is made on two time frames breaking our ability to directly compare simulation values with step selection outputs.
Further, the simulated preference of the animal is balanced against other demands such as site fidelity, movement resistance, and avoidance of certain locations.
The closest analogous values we could generate came from running RSF and iSSF models directly on the simulated decisions made by the animal.
We ran these models for both time scales: movement decisions every time step, and destination decisions every behavioral state switch.
These model results, while not strictly comparable to the other outputs, help confirm the animal was correctly preferring the chosen habitat (bare in mind the animal had a no preference exploratory state that would weaken the effect), and support the use of the median estimate to explore the cause behind the most deviant estimates.



<!-- ### Validation -->
<!-- comparing estimates to a second year of data -->

## Hypotheses

Although the study is primarily exploratory, with its scope determined by practical consideration of computational time, we register a few broad hypotheses.
- increases in tracking duration will lead to more consistent habitat selection results
- the most accurate estimates of habitat selection will come from tracking frequencies that match closest to the frequency of the animal's selection
- the choice of method will have a larger impact on selection detection than the decisions within methods
- ISSF will be the most success method as it matches closest to the underlying simulation mechanism

<!-- ## Possible Additional Explorations -->
<!-- pop level or summary of individuals, the latter would dodge the need to vary sample size -->
<!-- Sample size impact on population level estimates of preference. -->
<!-- Exploring the same decisions with different simulations of animal movement. -->

# Acknowledgements

# Software availablity

We used R `r paste0("v.", version$major, ".", version$minor)` [@base] via RStudio v.`r rstudioapi::versionInfo()$version` [@rstudio], and used rmarkdown `r paste0("v.", packageVersion("rmarkdown"))` [@rmarkdown2022; @rmarkdown2018; @rmarkdown2020], bookdown `r paste0("v.", packageVersion("bookdown"))` [@bookdown2016; @R-bookdown], tinytex `r paste0("v.", packageVersion("tinytex"))` [@tinytex2019; @tinytex2022], and knitr `r paste0("v.", packageVersion("knitr"))` [@knitr2015; @knitr2014; @knitr2022] packages to generate type-set outputs.

@osfr

<!-- other packs here -->
We generated R package citations with the aid of grateful `r paste0("v.", packageVersion("grateful"))` [@grateful].

# Data availabilty



# References
